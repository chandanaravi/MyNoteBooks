{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0564aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'message'], dtype='object')\n",
      "<class 'tuple'>\n",
      "Columns : 2\n",
      "Rows:5572\n",
      "Data Frame contains 2 columns and 5572 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../DataSets/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"]\n",
    ")\n",
    "print(df.columns)\n",
    "print(type(df.shape))\n",
    "print(f\"Columns : {df.shape[1]}\" )\n",
    "print(f\"Rows:{df.shape[0]}\" )\n",
    "print(f\"Data Frame contains {df.shape[1]} columns and {df.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7746d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "{'over', 'that', 'wouldn', 'same', 'haven', 'any', \"shouldn't\", 'only', 'than', \"hasn't\", \"it'll\", 'o', 'whom', \"they've\", 'when', 'shouldn', \"she's\", 'been', \"we've\", 'if', \"i'll\", 'ma', 'then', \"wouldn't\", 'yourself', 'at', 'having', 'did', 'other', 'to', 'are', 'will', 'should', 'above', 'which', 'doesn', 'hers', 'she', 'we', \"they'll\", 'll', 'just', 'what', 'hadn', 'their', 'me', 'he', 'don', \"he'd\", 'his', \"i'd\", 'own', 'some', 'between', 'had', 'd', 'weren', 'by', 'how', 'they', 'it', 'being', \"doesn't\", \"she'll\", \"you'll\", 'now', 'shan', \"he's\", 'my', 'once', 'herself', \"haven't\", 'few', 'them', 'her', 'have', 'after', 'ourselves', 'as', 'where', \"we're\", \"they're\", 'while', 'm', 'these', 'wasn', 'couldn', 'mightn', 'your', \"won't\", 'itself', 'an', 'for', 'no', 'myself', \"weren't\", 'there', 'off', \"we'll\", 'is', 'be', 'under', \"you've\", 'was', 'a', 'before', \"he'll\", \"that'll\", 'isn', \"wasn't\", 'too', 'and', 'so', 'below', 'but', 'ours', 'very', 'the', 'i', 'more', 'aren', 'does', 'doing', 'him', 'from', \"isn't\", \"she'd\", 'each', \"we'd\", \"it'd\", 'because', \"didn't\", 'out', \"couldn't\", \"aren't\", \"shan't\", 'with', 've', 'again', 're', 'were', \"they'd\", 'who', 'all', 'this', 'its', 't', 'through', 'of', 'those', 'theirs', 'themselves', 'won', 'why', \"don't\", \"it's\", \"mustn't\", 's', 'himself', 'both', 'down', \"mightn't\", \"i've\", 'ain', 'most', 'not', 'or', 'on', 'you', 'can', 'here', 'yours', 'during', 'in', 'hasn', \"i'm\", 'nor', \"you're\", 'didn', 'needn', \"should've\", 'against', \"hadn't\", 'into', 'up', 'until', 'y', 'about', 'such', \"needn't\", 'our', 'yourselves', 'further', 'do', 'am', 'mustn', \"you'd\", 'has'}\n",
      "len is 198\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(type(stop_words))\n",
    "print(stop_words)\n",
    "print(f\"len is {len(stop_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cf91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence showing stopword removal.\"\n",
    "text1=\"I'm loving NLP! Email me at test@email.com ðŸ˜Š\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1bc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop word from sentence.\n",
    "# Sample text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(type(tokens))\n",
    "print(tokens)\n",
    "print(text.split())\n",
    "print(text)\n",
    "\n",
    "print(\"==========================================================================\")\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(type(filtered_tokens))\n",
    "print(f\"Filtered tokens after removing stop words: {filtered_tokens}\" )\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820c65b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\", 'loving', 'NLP', '!', 'Email', 'me', 'at', 'test@email.com', 'ðŸ˜Š']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I'm loving NLP! Email me at test@email.com ðŸ˜Š\")\n",
    "[token.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa5161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['i', \"'\", 'm', 'loving', 'nl', '##p', '!', 'email', 'me', 'at', 'test', '@', 'email', '.', 'com', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens=tokenizer.tokenize(\"I'm loving NLP! Email me at test@email.com ðŸ˜Š\")\n",
    "print(type(tokens))\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "830f2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94cb7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
